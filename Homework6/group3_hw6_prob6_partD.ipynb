{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "# Our numerical workhorses\n",
    "import numpy as np\n",
    "from numpy import trapz\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import scipy.special\n",
    "\n",
    "# The MCMC Hammer\n",
    "import emcee\n",
    "\n",
    "# BE/Bi 103 utilities\n",
    "import bebi103\n",
    "\n",
    "# Import plotting tools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import corner\n",
    "\n",
    "# Magic function to make matplotlib inline; other style specs must come AFTER\n",
    "%matplotlib inline\n",
    "\n",
    "# This enables high res graphics inline (only use with static plots (non-Bokeh))\n",
    "# SVG is preferred, but there is a bug in Jupyter with vertical lines\n",
    "%config InlineBackend.figure_formats = {'png', 'retina'}\n",
    "\n",
    "# JB's favorite Seaborn settings for notebooks\n",
    "rc = {'lines.linewidth': 2, \n",
    "      'axes.labelsize': 18, \n",
    "      'axes.titlesize': 18, \n",
    "      'axes.facecolor': 'DFDFE5'}\n",
    "sns.set_context('notebook', rc=rc)\n",
    "sns.set_style('darkgrid', rc=rc)\n",
    "\n",
    "# Suppress future warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_likelihood(tau, t, m):\n",
    "    \"\"\"\n",
    "    Takes in tau (should be an array of length m), \n",
    "    data (Pandas data series), various values for m. \n",
    "    We are very proud of this function.\n",
    "    \"\"\"   \n",
    "    # set up array to store the log likelihoods calculated for each\n",
    "    # value of t\n",
    "    log_like = np.empty((len(t), 1))\n",
    "    \n",
    "    num_handler = np.empty((len(t), m)) # the numerator of the scaling factor\n",
    "    exp_handler = np.empty((len(t), m)) # the exponent part of the numerators\n",
    "    den_elem = np.empty(m) # temporary array to store elements of the denominator \n",
    "                           # of the scaling factor\n",
    "    den_handler = np.empty((len(t), m)) # denominator of the scaling factor\n",
    "    \n",
    "    # iterate over j for all data points \n",
    "    for j in range(m):\n",
    "        num_handler[:,j] = tau[j]**(m-2)\n",
    "        exp_handler[:,j] = -t/tau[j]\n",
    "        den_elem = tau[j] - tau\n",
    "        # use only non-zero elements when taking the product \n",
    "        # in the denominator of the scaling factor \n",
    "        den_prod = np.prod(den_elem[:j]) * np.prod(den_elem[j+1:])\n",
    "        den_handler[:,j] = den_prod \n",
    "        den_elem = np.empty(m)\n",
    "\n",
    "    scaling_factor = num_handler / den_handler\n",
    "\n",
    "    # store the log likelihoods in an array \n",
    "    log_like = scipy.misc.logsumexp(exp_handler, axis=1, b=scaling_factor)            \n",
    "\n",
    "    # return the sum of the log likelihoods\n",
    "    return np.sum(log_like)\n",
    "\n",
    "def log_prior(tau, m, tau_min, tau_max):\n",
    "    \"\"\"\n",
    "    Log prior for model defined above. Takes in m and tau, \n",
    "    returns log prior.\n",
    "    \"\"\"\n",
    "    # check that tau is ordered least to greatest\n",
    "    for i in range(m-1):\n",
    "        if not (tau[i] + 1e-6) < tau[i+1]:\n",
    "            return -np.inf\n",
    "    \n",
    "    # check that tau is not outside the boundaries we define\n",
    "    if tau[0] < tau_min or tau[-1] > tau_max:\n",
    "        return -np.inf\n",
    "    \n",
    "    prior = np.empty(m)\n",
    "    # calculate and store the elements of the normalized prior \n",
    "    # associated with each tau \n",
    "    for i in range(m):\n",
    "        if m == 1:\n",
    "            prior[i] = tau[i] * np.log(tau_max / tau_min)\n",
    "        \n",
    "        elif m > 1 and i == 0:\n",
    "            prior[i] = tau[i] * np.log(tau[i+1] / tau_min)\n",
    "        \n",
    "        elif m > 1 and i == m-1:\n",
    "            prior[i] = tau[i] * np.log(tau_max / tau[i-1])\n",
    "        \n",
    "        else:\n",
    "            prior[i] = tau[i] * np.log(tau[i+1] / tau[i-1])\n",
    "            \n",
    "    return -np.log(np.prod(prior))\n",
    "        \n",
    "def tau_start(m, n_walkers, n_temps):\n",
    "    \"\"\"\n",
    "    Generates starting points for each tau.\n",
    "    \"\"\"\n",
    "    p = np.empty((n_temps, n_walkers, m))\n",
    "    \n",
    "    for i in range(m):\n",
    "        p[:,:,i] = np.random.exponential(300, (n_temps,n_walkers))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_ptmcmc(data, m, model, tau_min=1, tau_max=1800, n_temps=20, n_walkers=100, n_burn=100, \n",
    "                  n_steps=500, threads=None):\n",
    "    \"\"\"\n",
    "    Sample posterior using PTMCMC.\n",
    "    \"\"\"\n",
    "    # arguments for likelihood\n",
    "    loglargs = (data, m)\n",
    "    \n",
    "    # arguments for prior\n",
    "    logpargs = (m, tau_min, tau_max)\n",
    "    \n",
    "    # starting points for the parameters\n",
    "    p0 = tau_start(m, n_walkers, n_temps)\n",
    "    \n",
    "    # column headings for outputted DataFrames\n",
    "    columns = {'m = 1': ['tau_1'],\n",
    "               'm = 2': ['tau_1', 'tau_2'],\n",
    "               'm = 3': ['tau_1', 'tau_2', 'tau_3'],\n",
    "               'm = 4': ['tau_1', 'tau_2', 'tau_3', 'tau_4'],\n",
    "               'm = 5': ['tau_1', 'tau_2', 'tau_3', 'tau_4', 'tau_5']}\n",
    "    \n",
    "    return bebi103.run_pt_emcee(\n",
    "            log_likelihood, log_prior, n_burn, n_steps,\n",
    "            n_temps=n_temps, p0=p0, loglargs=loglargs, logpargs=logpargs, \n",
    "            threads=threads, columns=columns[model], return_lnZ=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_posterior(p, t, m=3, tau_min=1, tau_max=1800):\n",
    "    \"\"\"\n",
    "    Calculates the log posterior using our best model.\n",
    "    \"\"\"\n",
    "    tau_1, tau_2, tau_3 = p\n",
    "    \n",
    "    # check that tau is ordered least to greatest\n",
    "    if not (tau_1 + 1e-6) < tau_2 and (tau_2 + 1e-6) < tau_3:\n",
    "        return -np.inf\n",
    "    \n",
    "    # check that tau is not outside the boundaries we define\n",
    "    if tau_1 < tau_min or tau_3 > tau_max:\n",
    "        return -np.inf\n",
    "    \n",
    "    return log_likelihood(p, t, m) + log_prior(p, m, tau_min, tau_max)\n",
    "\n",
    "def sample_mcmc(data, n_walkers=100, n_burn=100, \n",
    "                  n_steps=500, threads=None):\n",
    "    \"\"\"\n",
    "    Sample posterior using MCMC. Written for use\n",
    "    with our best model.\n",
    "    \"\"\"\n",
    "    #p0[:,:,0] = np.random.exponential(300, (n_walkers, 3)) \n",
    "    p0 = np.empty((n_walkers, 3))\n",
    "    p0[:,:] = np.array([10, 20, 30])\n",
    "    columns = ['tau_1', 'tau_2', 'tau_3'] \n",
    "\n",
    "    args = (data,)\n",
    "\n",
    "    return bebi103.run_ensemble_emcee(log_posterior, n_burn=n_burn, \n",
    "                                      n_steps=n_steps, n_walkers=n_walkers, \n",
    "                                      p0=p0, columns=columns, \n",
    "                                      args=args, threads=3)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>12 uM</th>\n",
       "      <th>7 uM</th>\n",
       "      <th>9 uM</th>\n",
       "      <th>10 uM</th>\n",
       "      <th>14 uM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25.000</td>\n",
       "      <td>35</td>\n",
       "      <td>25</td>\n",
       "      <td>50</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40.000</td>\n",
       "      <td>45</td>\n",
       "      <td>40</td>\n",
       "      <td>60</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.000</td>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>60</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45.429</td>\n",
       "      <td>50</td>\n",
       "      <td>45</td>\n",
       "      <td>75</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50.000</td>\n",
       "      <td>55</td>\n",
       "      <td>50</td>\n",
       "      <td>75</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    12 uM  7 uM  9 uM  10 uM  14 uM\n",
       "0  25.000    35    25     50     60\n",
       "1  40.000    45    40     60     75\n",
       "2  40.000    50    40     60     75\n",
       "3  45.429    50    45     75     85\n",
       "4  50.000    55    50     75    115"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/gardner_hw6/gardner_mt_catastrophe_only_tubulin.csv\", comment=\"#\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "df_7uM, lnZ_7uM, dlnZ_7uM = sample_ptmcmc(df['7 uM'].dropna(), 3, 'm = 3', \n",
    "                                    n_temps=20, n_walkers=100, \n",
    "                                    n_burn=1000, n_steps=5000, threads=3)\n",
    "print('The first PTMCMC took ', time.time()-start, ' seconds.')\n",
    "\n",
    "start = time.time()\n",
    "df_9uM, lnZ_9uM, dlnZ_9uM = sample_ptmcmc(df['9 uM'].dropna(), 3, 'm = 3', \n",
    "                                    n_temps=20, n_walkers=100, \n",
    "                                    n_burn=1000, n_steps=5000, threads=3)\n",
    "print('The second PTMCMC took ', time.time()-start, ' seconds.')\n",
    "\n",
    "start = time.time()\n",
    "df_10uM, lnZ_10uM, dlnZ_10uM = sample_ptmcmc(df['10 uM'].dropna(), 3, 'm = 3', \n",
    "                                    n_temps=20, n_walkers=100, \n",
    "                                    n_burn=1000, n_steps=5000, threads=3)\n",
    "print('The third PTMCMC took ', time.time()-start, ' seconds.')\n",
    "\n",
    "start = time.time()\n",
    "df_14uM, lnZ_14uM, dlnZ_14uM = sample_ptmcmc(df['14 uM'].dropna(), 3, 'm = 3', \n",
    "                                    n_temps=20, n_walkers=100, \n",
    "                                    n_burn=1000, n_steps=5000, threads=3)\n",
    "print('The fourth PTMCMC took ', time.time()-start, ' seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df7uM_map = df_7uM[(df_7uM.beta_ind==0) & (df_7uM.lnlike != -np.inf)]\n",
    "\n",
    "tau_map_7uM = np.array([df7uM_map['tau_1'].median(), df7uM_map['tau_2'].median(),\n",
    "                     df7uM_map['tau_3'].median()])\n",
    "\n",
    "post7uM = post_plot(tau_map1, np.arange(1800), 1, 1, 1800)\n",
    "post7uM_area = trapz(post1, dx=1)\n",
    "post7uM_norm = post1 / post1_area\n",
    "post7uM_cdf = np.cumsum(post1_norm)\n",
    "\n",
    "df7_dropna = df['7 uM'].dropna()\n",
    "\n",
    "# Build ECDF\n",
    "y = np.arange(len(df7_dropna)) / len(df7_dropna)\n",
    "x = np.sort(df7_dropna.values)\n",
    "\n",
    "# Plot ECDF and theoretical CDFs for different values of m\n",
    "plt.plot(np.arange(1800), post7uM_cdf, color='r')\n",
    "plt.plot(x, y, '.', color='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
