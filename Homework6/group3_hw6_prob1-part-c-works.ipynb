{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "# Our numerical workhorses\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import scipy.special\n",
    "\n",
    "# The MCMC Hammer\n",
    "import emcee\n",
    "\n",
    "# BE/Bi 103 utilities\n",
    "import bebi103\n",
    "\n",
    "# Import plotting tools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import corner\n",
    "\n",
    "# Magic function to make matplotlib inline; other style specs must come AFTER\n",
    "%matplotlib inline\n",
    "\n",
    "# This enables high res graphics inline (only use with static plots (non-Bokeh))\n",
    "# SVG is preferred, but there is a bug in Jupyter with vertical lines\n",
    "%config InlineBackend.figure_formats = {'png', 'retina'}\n",
    "\n",
    "# JB's favorite Seaborn settings for notebooks\n",
    "rc = {'lines.linewidth': 2, \n",
    "      'axes.labelsize': 18, \n",
    "      'axes.titlesize': 18, \n",
    "      'axes.facecolor': 'DFDFE5'}\n",
    "sns.set_context('notebook', rc=rc)\n",
    "sns.set_style('darkgrid', rc=rc)\n",
    "\n",
    "# Suppress future warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/gardner_hw6/gardner_mt_catastrophe_only_tubulin.csv\", comment=\"#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>12 uM</th>\n",
       "      <th>7 uM</th>\n",
       "      <th>9 uM</th>\n",
       "      <th>10 uM</th>\n",
       "      <th>14 uM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25.000</td>\n",
       "      <td>35</td>\n",
       "      <td>25</td>\n",
       "      <td>50</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40.000</td>\n",
       "      <td>45</td>\n",
       "      <td>40</td>\n",
       "      <td>60</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.000</td>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>60</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45.429</td>\n",
       "      <td>50</td>\n",
       "      <td>45</td>\n",
       "      <td>75</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50.000</td>\n",
       "      <td>55</td>\n",
       "      <td>50</td>\n",
       "      <td>75</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    12 uM  7 uM  9 uM  10 uM  14 uM\n",
       "0  25.000    35    25     50     60\n",
       "1  40.000    45    40     60     75\n",
       "2  40.000    50    40     60     75\n",
       "3  45.429    50    45     75     85\n",
       "4  50.000    55    50     75    115"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def log_likelihood(tau, t, m):\n",
    "    \"\"\"\n",
    "    Takes in tau (should be an array of length m), \n",
    "    data (Pandas data series), various values for m. \n",
    "    We are very proud of this function.\n",
    "    \"\"\"   \n",
    "    # set up array to store the log likelihoods calculated for each\n",
    "    # value of t\n",
    "    log_like = np.empty((len(t), 1))\n",
    "    \n",
    "    num_handler = np.empty((len(t), m)) # the numerator of the scaling factor\n",
    "    exp_handler = np.empty((len(t), m)) # the exponent part of the numerators\n",
    "    den_elem = np.empty(m) # temporary array to store elements of the denominator \n",
    "                           # of the scaling factor\n",
    "    den_handler = np.empty((len(t), m)) # denominator of the scaling factor\n",
    "    \n",
    "    # iterate over j for all data points \n",
    "    for j in range(m):\n",
    "        num_handler[:,j] = tau[j]**(m-2)\n",
    "        exp_handler[:,j] = -t/tau[j]\n",
    "        den_elem = tau[j] - tau\n",
    "        # use only non-zero elements when taking the product \n",
    "        # in the denominator of the scaling factor \n",
    "        den_prod = np.prod(den_elem[:j]) * np.prod(den_elem[j+1:])\n",
    "        den_handler[:,j] = den_prod \n",
    "        den_elem = np.empty(m)\n",
    "\n",
    "    scaling_factor = num_handler / den_handler\n",
    "\n",
    "    # store the log likelihoods in an array \n",
    "    log_like = scipy.misc.logsumexp(exp_handler, axis=1, b=scaling_factor)            \n",
    "\n",
    "    # return the sum of the log likelihoods\n",
    "    return np.sum(log_like)\n",
    "\n",
    "def log_prior(tau, m, tau_min, tau_max):\n",
    "    \"\"\"\n",
    "    Log prior for model defined above. Takes in m and tau, \n",
    "    returns log prior.\n",
    "    \"\"\"\n",
    "    # check that tau is ordered least to greatest\n",
    "    for i in range(m-1):\n",
    "        if not (tau[i] + 1e-6) < tau[i+1]:\n",
    "            return -np.inf\n",
    "    \n",
    "    # check that tau is not outside the boundaries we define\n",
    "    if tau[0] < tau_min or tau[-1] > tau_max:\n",
    "        return -np.inf\n",
    "    \n",
    "    prior = np.empty(m)\n",
    "    # calculate and store the elements of the normalized prior \n",
    "    # associated with each tau \n",
    "    for i in range(m):\n",
    "        if m == 1:\n",
    "            prior[i] = tau[i] * np.log(tau_max / tau_min)\n",
    "        \n",
    "        elif m > 1 and i == 0:\n",
    "            prior[i] = tau[i] * np.log(tau[i+1] / tau_min)\n",
    "        \n",
    "        elif m > 1 and i == m-1:\n",
    "            prior[i] = tau[i] * np.log(tau_max / tau[i-1])\n",
    "        \n",
    "        else:\n",
    "            prior[i] = tau[i] * np.log(tau[i+1] / tau[i-1])\n",
    "            \n",
    "    return -np.log(np.prod(prior))\n",
    "        \n",
    "def tau_start(m, n_walkers, n_temps):\n",
    "    \"\"\"\n",
    "    Generates starting points for each tau.\n",
    "    \"\"\"\n",
    "    p = np.empty((n_temps, n_walkers, m))\n",
    "    \n",
    "    for i in range(m):\n",
    "        p[:,:,i] = np.random.exponential(300, (n_temps,n_walkers))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_ptmcmc(data, m, model, tau_min=1, tau_max=1800, n_temps=20, n_walkers=100, n_burn=100, \n",
    "                  n_steps=500, threads=None):\n",
    "    \n",
    "    # arguments for likelihood\n",
    "    loglargs = (data, m)\n",
    "    \n",
    "    # arguments for prior\n",
    "    logpargs = (m, tau_min, tau_max)\n",
    "    \n",
    "    # starting points for the parameters\n",
    "    p0 = tau_start(m, n_walkers, n_temps)\n",
    "    \n",
    "    # column headings for outputted DataFrames\n",
    "    columns = {'m = 1': ['tau_1'],\n",
    "               'm = 2': ['tau_1', 'tau_2'],\n",
    "               'm = 3': ['tau_1', 'tau_2', 'tau_3'],\n",
    "               'm = 4': ['tau_1', 'tau_2', 'tau_3', 'tau_4'],\n",
    "               'm = 5': ['tau_1', 'tau_2', 'tau_3', 'tau_4', 'tau_5']}\n",
    "    \n",
    "    return bebi103.run_pt_emcee(\n",
    "            log_likelihood, log_prior, n_burn, n_steps,\n",
    "            n_temps=n_temps, p0=p0, loglargs=loglargs, logpargs=logpargs, \n",
    "            threads=threads, columns=columns[model], return_lnZ=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"//anaconda/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"//anaconda/lib/python3.5/multiprocessing/pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"//anaconda/lib/python3.5/site-packages/emcee/ptsampler.py\", line 91, in __call__\n    lp = self.logp(x, *self.logpargs, **self.logpkwargs)\n  File \"<ipython-input-4-dde625cdc6c5>\", line 55, in log_prior\n    prior[i] = tau[i] * np.log(tau[i+1] / tau_min)\nIndexError: index 1 is out of bounds for axis 0 with size 1\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-056d4544fb93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m df_1, lnZ_1, dlnZ_1 = sample_ptmcmc(df['12 uM'], 1, 'm = 1', \n\u001b[1;32m      3\u001b[0m                                     \u001b[0mn_temps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_walkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                                     n_burn=1000, n_steps=5000, threads=3)\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The first PTMCMC took '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' seconds.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-44fa3d755b4f>\u001b[0m in \u001b[0;36msample_ptmcmc\u001b[0;34m(data, m, model, tau_min, tau_max, n_temps, n_walkers, n_burn, n_steps, threads)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mlog_likelihood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_burn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mn_temps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_temps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloglargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloglargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogpargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogpargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             threads=threads, columns=columns[model], return_lnZ=True)\n\u001b[0m",
      "\u001b[0;32m/Users/elenaperry/Desktop/GitHub/DataAnalysisHomeworkRepository/Homework6/bebi103.py\u001b[0m in \u001b[0;36mrun_pt_emcee\u001b[0;34m(log_like, log_prior, n_burn, n_steps, n_temps, n_walkers, p_dict, p0, columns, loglargs, logpargs, threads, return_lnZ, return_sampler)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[0;31m# Do burn-in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m     \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_mcmc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_burn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorechain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;31m# Sample again, starting from end burn-in state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/emcee/sampler.py\u001b[0m in \u001b[0;36mrun_mcmc\u001b[0;34m(self, pos0, N, rstate0, lnprob0, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[1;32m    156\u001b[0m         for results in self.sample(pos0, lnprob0, rstate0, iterations=N,\n\u001b[0;32m--> 157\u001b[0;31m                                    **kwargs):\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/emcee/ptsampler.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, p0, lnprob0, lnlike0, iterations, thin, storechain)\u001b[0m\n\u001b[1;32m    257\u001b[0m                 \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m                 \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             logls = np.array([r[0] for r in results]).reshape((self.ntemps,\n",
      "\u001b[0;32m//anaconda/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         '''\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df_1, lnZ_1, dlnZ_1 = sample_ptmcmc(df['12 uM'], 1, 'm = 1', \n",
    "                                    n_temps=20, n_walkers=100, \n",
    "                                    n_burn=1000, n_steps=5000, threads=3)\n",
    "print('The first PTMCMC took ', time.time()-start, ' seconds.')\n",
    "\n",
    "start = time.time()\n",
    "df_2, lnZ_2, dlnZ_2 = sample_ptmcmc(df['12 uM'], 2, 'm = 2', \n",
    "                                    n_temps=20, n_walkers=100, \n",
    "                                    n_burn=1000, n_steps=5000, threads=3)\n",
    "print('The second PTMCMC took ', time.time()-start, ' seconds.')\n",
    "\n",
    "start = time.time()\n",
    "df_3, lnZ_3, dlnZ_3 = sample_ptmcmc(df['12 uM'], 3, 'm = 3', \n",
    "                                    n_temps=20, n_walkers=100, \n",
    "                                    n_burn=1000, n_steps=5000, threads=3)\n",
    "print('The third PTMCMC took ', time.time()-start, ' seconds.')\n",
    "\n",
    "start = time.time()\n",
    "df_4, lnZ_4, dlnZ_4 = sample_ptmcmc(df['12 uM'], 4, 'm = 4', \n",
    "                                    n_temps=20, n_walkers=100, \n",
    "                                    n_burn=1000, n_steps=5000, threads=3)\n",
    "print('The fourth PTMCMC took ', time.time()-start, ' seconds.')\n",
    "\n",
    "start = time.time()\n",
    "df_5, lnZ_5, dlnZ_5 = sample_ptmcmc(df['12 uM'], 5, 'm = 5', \n",
    "                                    n_temps=20, n_walkers=100, \n",
    "                                    n_burn=1000, n_steps=5000, threads=3)\n",
    "print('The fifth PTMCMC took ', time.time()-start, ' seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
