{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Problem 1.3** \n",
    "\n",
    "\n",
    "Before we do anything we should first import `numpy`, `pandas`, `matplotlib`, and `seaborn`. We are also going to use Justin's seaborn settings because he gave the class permission to. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "\n",
    "# This magic function lets us view plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "# JB's favorite Seaborn settings for notebooks\n",
    "rc={'lines.linewidth': 2, 'axes.labelsize': 18, 'axes.titlesize': 18,\n",
    "    'axes.facecolor': 'DFDFE5'}\n",
    "sns.set_context('paper', rc=rc)\n",
    "\n",
    "# Enable bokeh inline viewing\n",
    "\n",
    "#bokeh.io.output_notebook()\n",
    "\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part a** asks us to read the data from the data file into a `DataFrame`. As per the homework policies, the csv file is contained in a subdirectory called `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read the csv into a dataframe\n",
    "df = pd.read_csv('./data/gardner_et_al_2011_time_to_catastrophe_dic.csv', comment = '#')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part b** asks us if the data is tidy and then asks us to tidy the data. This data is not tidy because the columns (labeled tubulin and unlabeled tubulin) are not separate variables measured for the same observations, but rather two different sets of observations. Also, there are more data points for labeled tubulin than there are for unlabled tubulin. A way to clean up this data is to create two separate data sets. We ultilize `df.dropna()` key word argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# splice the data tables\n",
    "\n",
    "df_labeled = df['time to catastrophe with labeled tubulin (s)']\n",
    "df_unlabeled = df['time to catastrophe with unlabeled tubulin (s)']\n",
    "\n",
    "\n",
    "# Drop all NaN values\n",
    "df_unlabeled_clean = df['time to catastrophe with unlabeled tubulin (s)'].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Part c** asks us to plot a histogram of the castasrophe times for labeled and unlabeled tubulin. We used `plt.hist()`. Below are several ways we could have represented our data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = plt.hist(df_labeled, bins = 100, normed = False)\n",
    "_ = plt.hist(df_unlabeled_clean, bins = 100, normed = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = plt.hist(df_labeled, bins = 30, normed = False, histtype = 'stepfilled')\n",
    "_ = plt.hist(df_unlabeled_clean, bins = 30, normed = False, histtype = 'stepfilled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When playing around the the number of bins we found that too many bins, as shown in the first plot, made the plot difficult to interpret. The huge number of bins was unnecessary. We can see from the second plot that the `histtype = 'stepfilled'` does not represent the data well; we prefer the default `histtype` for our graph. \n",
    "\n",
    "Below is our final histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x10dd89710>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ = plt.hist(df_labeled, bins = 30, normed = False)\n",
    "_ = plt.hist(df_unlabeled_clean, bins = 30, normed = False, alpha = 0.5, color=\"red\")\n",
    "plt.xlabel('Time to Catastrophe (s)')\n",
    "plt.ylabel('Number of Catastrophes')\n",
    "plt.legend(('labeled', 'unlabeled'), loc='upper right', prop={\"size\":15})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow the viewer to see all of the data and easily compare the two data sets, we changed the transparancy of the histogram for the unlabeled data. The unlabeled histogram overlaps that of the labeled data. We did this using the `alpha` argument setting `alpha = 0.5`. We found that 30 bins represented the distribution of the data well, and we added a legend and axis labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part d** asks us to plot the cumulative histogram for the labeled and unlabeled experiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x10e08ee80>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#plot a cumulative  histogram\n",
    "_ = plt.hist(df_labeled, bins = 100, normed=1, cumulative = True, histtype = 'step', linewidth=2.0)\n",
    "_ = plt.hist(df_unlabeled_clean, normed=1, bins = 100, cumulative = True, histtype = 'step', linewidth=2.0,  color = 'red')\n",
    "\n",
    "plt.xlabel('Time to Catastrophe (s)')\n",
    "plt.ylabel('Cumulative Probability Distribution')\n",
    "plt.legend(('labeled', 'unlabeled'), loc='lower center', prop={\"size\":15})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the `cumulative` argument for `plt.hist()` to create this cumulative distribution plot for us. From a cumulative plot we want the sum of the data points to equal 1, with the y value of the previous data point being added to that of the current data point. In python, the `cumulative` argument for `plt.hist()` will add the previous value to the current value until it hits the last value; it does not manipulate the data so that total sum is equal to 1. This would not give us a probability distribution. In order to achieve this probability distribution, we have to set `normalized = True`. Unlike **part c** we felt that `histtype = 'step'` represented our data well. However, `histtype = 'stepfilled'` did not represent our data clearly; it was difficult to see the difference between the labeled and unlabled experiments, and the same goes for the default `histtype`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Question 2e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were asked to plot **cumulative histogram** similar to the ones that appear in Fig. 2a of the *Gardner, Zanic, et al.* paper. Dropping the \"NA\" values in order to get a tidy dataset came in handy since we were able to separate our dataset in to two dataframes, \"Labeled\" and \"Unlabeled.\"\n",
    "Cumulative frequencies have to be calculated before we can plot a histogram. Lets start by sorting the two dataframes in increasing order of time to catastrophe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_df_labeled = np.sort(df_labeled)\n",
    "sorted_df_unlabeled = np.sort(df_unlabeled_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataframes are sorted, we define a set of `timepoints` that we use to calculate cumulative frequencies. We use the `numpy` function `arange` to define a set of 30 values between the minimum and maximum time points at which we will calculate cumulative frequencies. We use the minimum and maximum values from the \"Unlabeled\" dataframe , within  `arange` function, based on its wider range. We added 30 to the maximum value to include the maximum time point from the \"Unlabeled\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timepoints = np.arange( np.min (df_unlabeled_clean), np.max(df_unlabeled_clean) +30, 30  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to store cumulative frequencies calculated at each time point as an array, we create an empty array using the `numpy` function `zeros`. The number of zeros depends on the length of the variable `timepoints`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "empty_array_labeled = np.zeros( len(timepoints) )\n",
    "empty_array_unlabeled = np.zeros ( len(timepoints) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We iterate over the number of `timepoints` using a for loop and using the in-built `sum` function, we calculate the number of entries that have catastrophe time points less than each value of the `timepoints` variable using boolean datatypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range( len(timepoints) ):\n",
    "    empty_array_labeled[i] = np.sum (sorted_df_labeled < timepoints[i])\n",
    "    empty_array_unlabeled [i] = np.sum (sorted_df_unlabeled < timepoints[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Cumulative Probability Distribution` function varies from 0 to 1. In order to normalize all cumulative frequencies to lie within this range, we divide all CFs by the maximum frequency that appears in \"Labeled\" and \"Unlabeled\" CFs, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalized_labeled = empty_array_labeled / np.max (empty_array_labeled)\n",
    "normalized_unlabeled = empty_array_unlabeled / np.max (empty_array_unlabeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the normalized cumulative frequencies in a dot-plot format and label the axis. Y axis limit was defined from 0 to 1.1 to include points with normalized cumulative frequency of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = plt.plot(timepoints, normalized_labeled , marker=\"o\", linestyle = \"None\")\n",
    "_ = plt.plot(timepoints, normalized_unlabeled, marker = \"o\", linestyle =\"None\", color=\"red\", alpha = 0.5)\n",
    "\n",
    "plt.xlabel('Time to Catastrophe (s)')\n",
    "plt.ylabel('Cumulative Probability Distribution')\n",
    "plt.ylim(0,1.1)\n",
    "# # Add a legend\n",
    "plt.legend(('labeled', 'unlabeled'), loc='lower right', prop={\"size\":15})\n",
    "\n",
    "plt.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
